from datasets import load_dataset

# Load the Wikitext-2 dataset
dataset = load_dataset("wikitext", "wikitext-2-raw-v1")# Inspect the dataset structure
print(dataset)
from transformers import AutoTokenizer

# Load the GPT-2 tokenizer
tokenizer = AutoTokenizer.from_pretrained("gpt2")
tokenizer.pad_token = tokenizer.eos_token  # Use EOS token for padding# Tokenize the dataset
def preprocess_function(examples):
    tokens = tokenizer(
        examples["text"], truncation=True, padding="max_length", max_length=128
    )
    tokens["labels"] = tokens["input_ids"].copy()  # Labels are the same as input_ids
    return tokens# Apply preprocessing
tokenized_dataset = dataset.map(preprocess_function, batched=True)
from transformers import GPT2LMHeadModel, GPT2Config

# Define the GPT-2 configuration
config = GPT2Config(
    vocab_size=len(tokenizer),  # Match tokenizerâ€™s vocabulary size
    n_embd=128,                # Embedding size
    n_layer=6,                 # Number of layers
    n_head=8                  # Number of attention heads
)# Initialize the model
model = GPT2LMHeadModel(config)
from transformers import TrainingArguments, Trainer

# Define training arguments
training_args = TrainingArguments(
    output_dir="./small-gpt2-model",  # Save the model here
    overwrite_output_dir=True,
    num_train_epochs=3,               # Number of epochs
    per_device_train_batch_size=8,    # Batch size per device
    save_steps=500,                   # Save model every 500 steps
    save_total_limit=2,               # Limit the number of saved checkpoints
    logging_dir="./logs",            # Logging directory
    logging_steps=100
)# Initialize the Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["validation"]
)
trainer.train()
